---
description: "Unified Lead Engine Architecture: BrainScraper + Scrapegoat Integration Map"
alwaysApply: true
globs:
  - "brainscraper/**"
  - "scrapegoat/**"
  - "docker-compose.yml"
  - "railway.toml"
---

# Unified Lead Engine Architecture - Complete Integration Map

## ğŸš€ MISSION ALPHA: TRIPLE-VESSEL SYSTEM

**STATUS: âœ… DEPLOYED AND OPERATIONAL**

The **Triple-Vessel Stealth Extraction Engine** consists of three isolated services:
- **The General** (`/brainscraper`) - Node.js/Next.js orchestrator âœ… Online
- **The Brain** (`/chimera_brain`) - Python 3.11 AI service âœ… Online
- **The Body** (`/chimera-core`) - Rust stealth worker swarm âœ… Online

**See:** `.cursor/rules/000-mission-alpha.mdc` for complete Mission Alpha documentation.

**Start System:** Use `./start-triple-vessel.sh` (local) or verify Railway services are online.

**Current Status:** All services deployed and healthy on Railway.

---

## ğŸ¯ CRITICAL: Unorthodox Architecture Understanding

**This is NOT a traditional monorepo.** This system combines **TWO SEPARATE CODEBASES** (`/brainscraper` and `/scrapegoat`) into a unified process via Railway deployment. They communicate through Redis, not direct code imports.

**NOTE:** Mission Alpha adds two new services (`/chimera_brain` and `/chimera-core`) that communicate via gRPC. The existing BrainScraper + Scrapegoat architecture remains intact and functional.

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAILWAY PLATFORM                          â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  BrainScraper    â”‚         â”‚   Scrapegoat      â”‚          â”‚
â”‚  â”‚  (Next.js)       â”‚         â”‚   (FastAPI)      â”‚          â”‚
â”‚  â”‚  Port: 3000      â”‚         â”‚   Port: 8000     â”‚          â”‚
â”‚  â”‚  Root: /brainscraperâ”‚      â”‚   Root: /scrapegoatâ”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚           â”‚                             â”‚                    â”‚
â”‚           â”‚  LPUSH leads_to_enrich      â”‚                    â”‚
â”‚           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                    â”‚
â”‚           â”‚                             â”‚                    â”‚
â”‚           â–¼                             â–¼                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚         Redis Bridge (Shared)              â”‚              â”‚
â”‚  â”‚  Queue: leads_to_enrich                    â”‚              â”‚
â”‚  â”‚  DLQ: failed_leads                         â”‚              â”‚
â”‚  â”‚  Hive Mind: Vector Memory (Redis Stack)     â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚           â”‚                             â”‚                    â”‚
â”‚           â”‚                             â”‚ BRPOP              â”‚
â”‚           â”‚                             â”‚                    â”‚
â”‚           â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚           â”‚         â”‚                                         â”‚
â”‚           â–¼         â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚      PostgreSQL (Shared Database)          â”‚              â”‚
â”‚  â”‚      Table: leads                          â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  Chimera Brain   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¤  Chimera Core     â”‚          â”‚
â”‚  â”‚  (Python 3.11)   â”‚  gRPC   â”‚  (Rust)          â”‚          â”‚
â”‚  â”‚  HTTP: 8080      â”‚         â”‚  Worker Swarm     â”‚          â”‚
â”‚  â”‚  gRPC: 50051     â”‚         â”‚  (Stealth)        â”‚          â”‚
â”‚  â”‚  Root: /chimera_brainâ”‚     â”‚  Root: /chimera-coreâ”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚           â”‚                                                    â”‚
â”‚           â”‚ Redis (Hive Mind)                                 â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ THE 5-PHASE PROCESS (Your Map to Success)

### Phase 1: Lead Discovery (The Producer)
**Service:** BrainScraper.io (`/brainscraper`)
**Location:** `brainscraper/app/components/LinkedInLeadGenerator.tsx` or `FacebookLeadGenerator.tsx`

**Process (Primary Path - In-Memory Enrichment):**
1. User initiates search via Next.js UI (LinkedIn Sales Navigator or Facebook Discovery)
2. System calls `premium_search_person` endpoint via RapidAPI
   - API: `https://realtime-linkedin-sales-navigator-data.p.rapidapi.com/premium_search_person`
   - Headers: `x-rapidapi-key`, `x-rapidapi-host`
   - Uses `RAPIDAPI_KEY` environment variable
3. **Post-Filter Validation:** Ensures leads match exact requested location text before proceeding
4. **In-Memory Enrichment:** User clicks "Enrich" â†’ `enrichData()` function processes leads synchronously
   - Calls skip-tracing APIs, Telnyx, USHA DNC, Census APIs
   - Saves to `enriched-all-leads.json` via `/api/aggregate-enriched-leads`
   - **Note:** BrainScraper does NOT push to Redis queue - it uses in-memory enrichment

**Key Files:**
- `brainscraper/app/components/LinkedInLeadGenerator.tsx` - LinkedIn scraping UI
- `brainscraper/app/components/FacebookLeadGenerator.tsx` - Facebook discovery UI
- `brainscraper/utils/enrichData.ts` - In-memory enrichment function
- `brainscraper/app/api/aggregate-enriched-leads/route.ts` - Saves to JSON file
- `brainscraper/utils/scrapeUsageTracker.ts` - Track daily/monthly usage
- `brainscraper/utils/cooldownManager.ts` - Error spike detection

**Alternative Path (Scrapegoat Queue-Based):**
- Scrapegoat API endpoint `/worker/process-lead` can push to Redis queue `leads_to_enrich`
- Scrapegoat workers consume from queue and save to PostgreSQL
- This is a separate, alternative enrichment path

---

### Phase 2: The Redis Bridge (The Buffer)
**Infrastructure:** `redis-bridge` (Railway Shared Variable)

**Role:**
- Acts as "Holding Bay" for raw lead data
- Decouples high-speed discovery (Phase 1) from intensive AI processing (Phase 3)
- **Critical:** If enrichment workers crash, no leads are lost (persistent queue)

**Queue Names:**
- `leads_to_enrich` - Main queue (FIFO)
- `failed_leads` - Dead Letter Queue (DLQ) for failed enrichments after 3 retries

**Format:**
```json
{
  "linkedinUrl": "https://linkedin.com/in/...",
  "name": "John Doe",
  "location": "Naples, Florida, United States",
  "title": "Software Engineer",
  "company": "Acme Corp",
  "platform": "linkedin",
  "sourceDetails": { ... }
}
```

**Environment Variables (Both Services):**
- `REDIS_URL=${{redis-bridge.REDIS_URL}}` - Railway shared variable syntax

---

### Phase 3: The Enrichment Swarm (The Consumer)
**Service:** Scrapegoat Worker (`/scrapegoat`)
**Location:** `scrapegoat/app/workers/redis_queue_worker.py` (or `start_redis_worker.py`)

**Process:**
1. **Polling:** 5+ autonomous "Bees" (worker replicas) constantly run `BRPOP` on `leads_to_enrich` queue
2. **Identity Resolution:** Resolves raw LinkedIn data into verifiable US identity
3. **Skip-Tracing:** Calls Skip-Tracing API to find valid phone numbers and emails
   - API: `https://skip-tracing-working-api.p.rapidapi.com/search/byemail` or `/bynameaddress`
   - Uses `RAPIDAPI_KEY` or `RAPIDAPI_KEY_SKIP_TRACING`
4. **Telnyx Gatekeep:** Validates phone number
   - If VOIP/Landline or junk carrier â†’ **STOP HERE** (save costs)
   - Only mobile numbers proceed
5. **USHA DNC Scrub:** Checks number against Do-Not-Call registry
   - API: `https://api-business-agent.ushadvisors.com/Leads/api/leads/scrubphonenumber`
   - Uses `USHA_JWT_TOKEN` or `COGNITO_REFRESH_TOKEN`
6. **Demographic Enrichment:** If lead passes Gatekeep, pulls US census data
   - Median Income, Age, Address
   - API: Census API or `household-income-by-zip-code.p.rapidapi.com`
   - Uses `CENSUS_API_KEY`
7. **The Trauma Center (Self-Healing):** If public record sites change UI
   - Vision LLMs (OpenAI) autonomously re-map selectors
   - Uses `OPENAI_API_KEY` for self-healing capabilities

**Key Files:**
- `scrapegoat/app/workers/redis_queue_worker.py` - Main worker loop
- `scrapegoat/app/smartfields/patterns/` - Pattern recognition (address, email, phone, etc.)
- `scrapegoat/main.py` - FastAPI service (health checks, queue status)

**Retry Logic:**
- `MAX_RETRIES = 3` - Maximum retry attempts
- `RETRY_DELAY_BASE = 5` - Base delay in seconds (exponential backoff)
- After 3 failures â†’ Push to `failed_leads` DLQ

---

### Phase 4: Permanent Memory (Persistence)
**Infrastructure:** PostgreSQL (Railway Shared Variable)

**Process:**
1. Once a "Bee" completes enrichment, writes final record to `leads` table in PostgreSQL
2. **Deduplication:** Uses unique key (LinkedIn URL or Name+Email+Phone) to prevent duplicate enrichment costs
3. **Fail-Safe (DLQ):** If lead fails enrichment 3 times â†’ moved to `failed_leads` queue
   - Can be inspected via Scrapegoat Internal UI
   - Manual retry via API: `POST /dlq/retry/{item_index}`

**Database Schema (Expected):**
```sql
CREATE TABLE leads (
  id SERIAL PRIMARY KEY,
  linkedin_url VARCHAR(255) UNIQUE,
  name VARCHAR(255),
  phone VARCHAR(20),
  email VARCHAR(255),
  city VARCHAR(100),
  state VARCHAR(50),
  zipcode VARCHAR(10),
  age INTEGER,
  income VARCHAR(50),
  dnc_status VARCHAR(20),
  can_contact BOOLEAN,
  enriched_at TIMESTAMP,
  created_at TIMESTAMP DEFAULT NOW()
);
```

**Environment Variables:**
- `DATABASE_URL=${{PostgreSQL.DATABASE_URL}}` - Railway shared variable

---

### Phase 5: Result Visualization (The Dashboard)
**Service:** BrainScraper.io (UI Layer)
**Location:** `brainscraper/app/enriched/page.tsx`

**Process:**
1. Next.js UI queries PostgreSQL database via `/api/load-enriched-results`
2. Displays final list of leads (target: 10,000 leads/week)
3. **Filters:**
   - Age â‰¤ 59
   - Income thresholds
   - "Safe to Call" status (DNC checked and passed)

**Key Files:**
- `brainscraper/app/enriched/page.tsx` - Main enriched leads viewer
- `brainscraper/app/api/load-enriched-results/route.ts` - API endpoint (if exists)
- `brainscraper/utils/extractLeadSummary.ts` - Lead summary extraction

---

## ğŸ”§ RAILWAY DEPLOYMENT REQUIREMENTS

### Critical Settings for Each Service

#### BrainScraper Service
**Railway Dashboard:**
- **Root Directory:** `brainscraper` (NOT root!)
- **Watch Paths:** `brainscraper/**`
- **Port:** `3000` (from env var)
- **Build Command:** `npm run build` (must use `--webpack` flag to avoid Turbopack errors)
- **Start Command:** `npm start`

**Environment Variables:**
```bash
NODE_ENV=production
PORT=3000
REDIS_URL=${{redis-bridge.REDIS_URL}}
DATABASE_URL=${{PostgreSQL.DATABASE_URL}}
NEXT_PUBLIC_BASE_URL=https://brainscraper.io
DATA_DIR=/data
RAPIDAPI_KEY=your-key
USHA_JWT_TOKEN=your-token
```

**Build Configuration:**
- Must use `next build --webpack` (NOT Turbopack)
- Custom config in `next.config.js` requires webpack

#### Scrapegoat Service
**Railway Dashboard:**
- **Root Directory:** `scrapegoat` (NOT root!)
- **Watch Paths:** `scrapegoat/**`
- **Port:** `8000` (auto-assigned)
- **Build Command:** `pip install -r requirements.txt && playwright install chromium`
- **Start Command:** `python main.py`

**Environment Variables:**
```bash
PYTHONUNBUFFERED=1
DATABASE_URL=${{PostgreSQL.DATABASE_URL}}
APP_DATABASE_URL=${{PostgreSQL.DATABASE_URL}}
REDIS_URL=${{redis-bridge.REDIS_URL}}
APP_REDIS_URL=${{redis-bridge.REDIS_URL}}
APP_CELERY_BROKER_URL=${{redis-bridge.REDIS_URL}}/1
APP_CELERY_RESULT_BACKEND=${{redis-bridge.REDIS_URL}}/2
OPENAI_API_KEY=sk-proj-...
CENSUS_API_KEY=b4f15ee777...
RAPIDAPI_KEY=your-key
```

#### Scrapegoat Worker Service (5+ Replicas)
**Railway Dashboard:**
- **Root Directory:** `scrapegoat` (same as main service)
- **Watch Paths:** `scrapegoat/**`
- **Start Command:** `python start_redis_worker.py`
- **Scaling:** 5 replicas (or more for higher throughput)

**Environment Variables:**
- Same as Scrapegoat service above

---

## ğŸ”— CRITICAL INTEGRATION POINTS

### 1. Redis Queue Format
**Note:** BrainScraper does NOT push to Redis. It uses in-memory enrichment. Only Scrapegoat uses Redis queues.

**Producer (Scrapegoat API):**
```python
# File: scrapegoat/main.py
@app.post("/worker/process-lead")
async def process_lead(lead_data: Dict[str, Any]):
    get_redis().lpush("leads_to_enrich", json.dumps(lead_data))
    return {"status": "queued"}
```

**Consumer (Scrapegoat):**
```python
# In scrapegoat (Python)
import redis
import json

redis_client = redis.from_url(os.getenv("REDIS_URL"))

# Worker loop
while True:
    result = redis_client.brpop('leads_to_enrich', timeout=10)
    if result:
        queue_name, lead_json = result
        lead_data = json.loads(lead_json)
        # Process lead...
```

### 2. PostgreSQL Write Format
**After Enrichment (Scrapegoat):**
```python
# Write to PostgreSQL
import psycopg2

conn = psycopg2.connect(os.getenv("DATABASE_URL"))
cur = conn.cursor()

cur.execute("""
    INSERT INTO leads (linkedin_url, name, phone, email, city, state, zipcode, age, income, dnc_status, can_contact, enriched_at)
    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW())
    ON CONFLICT (linkedin_url) DO NOTHING
""", (lead_data['linkedinUrl'], name, phone, email, city, state, zipcode, age, income, dnc_status, can_contact))

conn.commit()
```

### 3. BrainScraper Query Format
**Read from File System (BrainScraper):**
```typescript
// File: brainscraper/app/api/load-enriched-results/route.ts
// ACTUAL IMPLEMENTATION: Reads from file system, NOT PostgreSQL
import { getDataFilePath, safeReadFile } from '@/utils/dataDirectory';

const filePath = getDataFilePath('enriched-all-leads.json');
const content = safeReadFile(filePath);
const data = JSON.parse(content);
// Returns leads from JSON file, not database
```

**Note:** BrainScraper does NOT read from PostgreSQL. It reads from `enriched-all-leads.json` file.

---

## ğŸš¨ CRITICAL RULES & CONSTRAINTS

### Data Protection (See `.cursorrules`)
- **NEVER** overwrite existing enriched leads
- **NEVER** delete `enriched-all-leads.json`
- Always merge, never replace
- Use deduplication keys (LinkedIn URL or name+email+phone)

### Service Boundaries
- **BrainScraper** = Producer (scrapes, pushes to Redis)
- **Scrapegoat** = Consumer (pulls from Redis, enriches, writes to PostgreSQL)
- **NO direct code imports** between services
- **ONLY communication:** Redis queue + PostgreSQL database

### Railway Shared Variables
- Use `${{service-name.VARIABLE_NAME}}` syntax
- Both services MUST point to same `REDIS_URL` and `DATABASE_URL`
- This is how they "unify" - via shared infrastructure, not code

### Build Requirements
- BrainScraper: Must use `--webpack` flag (Turbopack incompatible with custom config)
- Scrapegoat: Must install Playwright (`playwright install chromium`)

---

## ğŸ“ KEY FILE LOCATIONS

### BrainScraper (`/brainscraper`)
- **Scraping UI:** `app/components/LinkedInLeadGenerator.tsx`, `FacebookLeadGenerator.tsx`
- **Settings:** `app/settings/page.tsx`
- **Enriched Viewer:** `app/enriched/page.tsx`
- **Usage Tracking:** `utils/scrapeUsageTracker.ts` (NEW)
- **Cooldown Manager:** `utils/cooldownManager.ts` (NEW)
- **API Routes:** `app/api/settings/route.ts`, `app/api/scrape-history/route.ts`
- **Redis Integration:** Should be in `lib/redis.ts` or `utils/redis.ts` (may need to create)

### Scrapegoat (`/scrapegoat`)
- **Main API:** `main.py` (FastAPI service)
- **Worker:** `start_redis_worker.py` or `app/workers/redis_queue_worker.py`
- **SmartFields:** `app/smartfields/patterns/` (address, email, phone, etc.)
- **Spiders:** `app/scraping/spiders/` (web scraping logic)
- **DLQ API:** `app/api/dlq.py` (Dead Letter Queue management)

### Shared Infrastructure
- **Docker Compose:** `docker-compose.yml` (local dev only)
- **Railway Config:** `brainscraper/railway.toml`, `scrapegoat/railway.toml`
- **Documentation:** `README_REDIS_BRIDGE.md`, `MONOREPO_RAILWAY_SETUP.md`

---

## âœ… SUCCESS CHECKLIST

When implementing features, verify:

1. âœ… **Service Isolation:** Code changes stay in correct service directory
2. âœ… **Redis Format:** Queue messages match expected JSON structure
3. âœ… **PostgreSQL Schema:** Database writes match expected table structure
4. âœ… **Railway Config:** Root directories and watch paths are correct
5. âœ… **Shared Variables:** Both services use `${{...}}` syntax for Redis/DB
6. âœ… **Data Protection:** Never overwrite existing enriched leads
7. âœ… **Build Flags:** BrainScraper uses `--webpack`, Scrapegoat installs Playwright
8. âœ… **Environment Variables:** All required keys are set in Railway
9. âœ… **Worker Scaling:** Scrapegoat worker has 5+ replicas for throughput
10. âœ… **Error Handling:** Failed leads go to DLQ after 3 retries

---

## ğŸ¯ REMEMBER

**This is an unorthodox architecture:** Two separate codebases unified via Railway shared infrastructure (Redis + PostgreSQL), NOT via code imports. The "unification" happens at the infrastructure level, not the code level.

**Your goal:** 10,000 enriched leads/week with Age â‰¤ 59, Income data, and "Safe to Call" status.

**Your map:** This document. Follow the 5 phases, respect service boundaries, and always protect existing enriched data.
